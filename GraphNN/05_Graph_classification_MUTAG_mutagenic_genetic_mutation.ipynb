{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "167f7ae2",
   "metadata": {},
   "source": [
    "## Binary Graph Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f591911",
   "metadata": {},
   "source": [
    "### Purpose\n",
    "\n",
    "Applying GNNs to the task of graph classification.\n",
    "\n",
    "The goal is to classify an entire graph instead of single nodes or edges. Therefore, we are also given a dataset of multiple graphs that we need to classify based on some structural graph properties. The most common task for graph classification is molecular property prediction, in which molecules are represented as graphs. Each atom is linked to a node, and edges in the graph are the bonds between atoms. For example, look at the figure below.\n",
    "\n",
    "<center width=\"100%\"><img src=\"./img/molecular_prop_graph_classification.png\" width=\"400px\"></center>\n",
    "\n",
    "On the left, we have an arbitrary, small molecule with different atoms, whereas the right part of the image shows the graph representation.\n",
    "\n",
    "\n",
    "### Node Embedding\n",
    "The atom types are abstracted as node features (e.g. a one-hot vector), and the different bond types are used as edge features. **For simplicity**, we will neglect the edge attributes in this tutorial, but we can include them by using methods like the Relational Graph Convolution that uses a different weight matrix for each edge type.\n",
    "\n",
    "\n",
    "### MUTAG Data Set\n",
    "The dataset we will use below is called the MUTAG dataset. It is a common small benchmark for graph classification algorithms, and _contain 188 graphs with 18 nodes and 20 edges on average for each graph_.\n",
    "\n",
    "The graphs have **2 different labels/atom types**, and the binary graph labels represent \"their mutagenic effect on a specific gram negative bacterium\" (the specific meaning of the labels are not too important here). **Each node is represented by an x_feature of size 7**.\n",
    "\n",
    "The dataset is part of a large collection of different graph classification datasets, known as the [TUDatasets](https://chrsmrrs.github.io/datasets/), which is directly accessible via torch_geometric.datasets ([documentation](https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html#torch_geometric.datasets.TUDataset))  in PyTorch Geometric. We can load the dataset below.\n",
    "\n",
    "Here is an example of a graph in MUTAG dataset:\n",
    "\n",
    "<center width=\"100%\"><img src=\"./img/MUTAG_dataset_example.png\" width=\"200px\"></center>\n",
    "\n",
    "### What MUTAG dataset represents for?\n",
    "In general, TU-Dataset is a collection of over 120 datasets of varying sizes from a wide range of applications. One of them is MUTAG. MUTAG is a commonly used dataset for evaluating graph classification algorithms. Each graph in the dataset represents a chemical compound and graph labels represent \"their mutagenic effect on a specific gram negative bacterium\". In genetics, a **mutagen is a physical or chemical agent that permanently changes genetic material, usually DNA, in an organism** and thus increases the frequency of mutations above the natural background level.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96cde86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fec82aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "import torch_geometric\n",
    "import torch_geometric.nn as geom_nn\n",
    "import torch_geometric.loader as loader\n",
    "from torch_geometric import utils # to convert the edge index to actual adjacent matrix for some preliminary implementations (not a good practice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5618f38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"./saved_models/test\"\n",
    "\n",
    "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "DATASET_PATH = \"./data\"\n",
    "\n",
    "num_workers = 2 # number of cpus to emply (for data loader function)\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9862857d",
   "metadata": {},
   "source": [
    "#### 1 Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32a99775",
   "metadata": {},
   "outputs": [],
   "source": [
    "tu_dataset = torch_geometric.datasets.TUDataset(root=DATASET_PATH, name=\"MUTAG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1faa11c",
   "metadata": {},
   "source": [
    "The first line shows how the dataset stores different graphs ([documentation](https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html#torch_geometric.data.Data)). The nodes, edges, and labels of each graph are concatenated to one tensor, and the dataset stores the indices where to split the tensors correspondingly. The length of the dataset is the number of graphs we have, and the \"average label\" denotes the percentage of the graph with label 1. As long as the percentage is in the range of 0.5, we have a relatively balanced dataset. It happens quite often that graph datasets are very imbalanced, hence checking the class balance is always a good thing to do.\n",
    "\n",
    "Next, we will split our dataset into a training and test part. Note that we do not use a validation set this time because of the small size of the dataset. Therefore, our model might overfit slightly on the validation set due to the noise of the evaluation, but we still get an estimate of the performance on untrained data.\n",
    "\n",
    "\n",
    "\n",
    "There are 3371 nodes in total for all 188 graphs. Therefore, there are $3371 / 188 =~ 18$ nodes for each graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf65198c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data object: Data(x=[3371, 7], edge_index=[2, 7442], edge_attr=[7442, 4], y=[188])\n",
      "Length: 188\n",
      "Average label: 0.66\n"
     ]
    }
   ],
   "source": [
    "print(\"Data object:\", tu_dataset.data)\n",
    "print(\"Length:\", len(tu_dataset))\n",
    "print(f\"Average label: {tu_dataset.data.y.float().mean().item():4.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ffd4132",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "tu_dataset.shuffle()\n",
    "train_dataset = tu_dataset[:150]\n",
    "test_dataset = tu_dataset[150:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd17089",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bf768f",
   "metadata": {},
   "source": [
    "#### 2 Batching philosophy \n",
    "\n",
    "When using a data loader, we encounter a problem with batching  N  graphs. Each graph in the batch can have a different number of nodes and edges, and hence we would require a lot of padding to obtain a single tensor. Torch geometric uses a different, more efficient approach: we can view the  N  graphs in a batch as a single large graph with concatenated node and edge list. As there is no edge between the  N  graphs, running GNN layers on the large graph gives us the same output as running the GNN on each graph separately. Visually, this batching strategy is visualized below ([tutorial here](https://colab.research.google.com/drive/1I8a0DfQ3fI7Njc62__mVXUlcAleUclnb?usp=sharing#scrollTo=2owRWKcuoALo))).\n",
    "\n",
    "\n",
    "<center width=\"100%\"><img src=\"./img/graph_mini_batching.png\" width=\"600px\"></center>\n",
    "\n",
    "\n",
    "The adjacency matrix is zero for any nodes that come from two different graphs, and otherwise according to the adjacency matrix of the individual graph. Luckily, this strategy is already implemented in torch geometric, and hence we can use the corresponding data loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3e70e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_train_loader = loader.DataLoader(train_dataset, batch_size=64, num_workers=num_workers, shuffle=True)\n",
    "graph_val_loader = loader.DataLoader(test_dataset, batch_size=64, num_workers=num_workers) # Additional loader if you want to change to a larger dataset\n",
    "graph_test_loader = loader.DataLoader(test_dataset, batch_size=64, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8feebe0",
   "metadata": {},
   "source": [
    "Let's load a batch below to see the batching in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d64782d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: DataBatch(edge_index=[2, 1512], x=[687, 7], edge_attr=[1512, 4], y=[38], batch=[687], ptr=[39])\n",
      "Labels: tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0])\n",
      "Batch indices: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 5, 5, 5, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(graph_test_loader))\n",
    "print(\"Batch:\", batch)\n",
    "print(\"Labels:\", batch.y[:10])\n",
    "print(\"Batch indices:\", batch.batch[:80])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f950fbd",
   "metadata": {},
   "source": [
    "We have 38 graphs stacked together for the test dataset (150 kept for train dataset). \n",
    "\n",
    "We have 2 labels / classes.\n",
    "\n",
    "The batch indices, stored in batch, show that the first 12 nodes belong to the first graph, the next 22 to the second graph, and so on. These indices are important for performing the final prediction. To perform a prediction over a whole graph, we usually perform a pooling operation over all nodes after running the GNN model. In this case, we will use the average pooling. Hence, we need to know which nodes should be included in which average pool. Using this pooling, we can already create our graph network below. Specifically, we re-use our class GNNModel from before, and simply add an average pool and single linear layer for the graph prediction task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb72989",
   "metadata": {},
   "source": [
    "### Defining various Layers\n",
    "\n",
    "Later, we will see that we can use the internal classes from torch-geometric incstead of defining them our selves. The following are the classes I wrote, equivalent to GCN and the Attention Layer. They use the actual Adjacent Matrix Rather than the edge-index which the torch-geometric uses and is more efficient  in terms of memory. So, the following two classes are not for final production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9a416c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are defined by myself for testing, and different that the internal classes defined in torch-geometric in:\n",
    "# 1) We use the actual adjacent matrix, rather than the edge-index. from memory saving practice this is not good.\n",
    "# But we do it for learning purposes\n",
    "class GCNLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, node_feats, edge_index):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            node_feats - Tensor with node features of shape [batch_size, num_nodes, c_in]\n",
    "            adj_matrix - Batch of adjacency matrices of the graph. If there is an edge from i to j, adj_matrix[b,i,j]=1 else 0.\n",
    "                         Supports directed edges by non-symmetric matrices. Assumes to already have added the identity connections.\n",
    "                         Shape: [batch_size, num_nodes, num_nodes]\n",
    "        \"\"\"\n",
    "        # Num neighbours = number of incoming edges\n",
    "        adj_matrix = utils.to_dense_adj(edge_index)\n",
    "        node_feats = node_feats.view(1, node_feats.shape[0], node_feats.shape[1]) # remove 1st dimension with is for old-way of representation of Batch\n",
    "        num_neighbours = adj_matrix.sum(dim=-1, keepdims=True)\n",
    "        node_feats = self.projection(node_feats)\n",
    "        node_feats = torch.bmm(adj_matrix, node_feats)\n",
    "        node_feats = node_feats / num_neighbours\n",
    "        return node_feats.squeeze() # remove the 1st dimension since it is for old-batching-style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc96b11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "869ba09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_heads=1, concat_heads=True, alpha=0.2):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            in_channels - Dimensionality of input features\n",
    "            out_channels - Dimensionality of output features\n",
    "            num_heads - Number of heads, i.e. attention mechanisms to apply in parallel. The\n",
    "                        output features are equally split up over the heads if concat_heads=True.\n",
    "            concat_heads - If True, the output of the different heads is concatenated instead of averaged.\n",
    "            alpha - Negative slope of the LeakyReLU activation.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.concat_heads = concat_heads\n",
    "        if self.concat_heads:\n",
    "            assert out_channels % num_heads == 0, \"Number of output features must be a multiple of the count of heads.\"\n",
    "            out_channels = out_channels // num_heads\n",
    "\n",
    "        # Sub-modules and parameters needed in the layer\n",
    "        self.projection = nn.Linear(in_channels, out_channels * num_heads)\n",
    "        self.a = nn.Parameter(torch.Tensor(num_heads, 2 * out_channels))  # One per head\n",
    "        self.leakyrelu = nn.LeakyReLU(alpha)\n",
    "\n",
    "        # Initialization from the original implementation\n",
    "        nn.init.xavier_uniform_(self.projection.weight.data, gain=1.414)\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "    def forward(self, node_feats, edge_index, print_attn_probs=False):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            node_feats - Input features of the node. Shape: [batch_size, c_in]\n",
    "            adj_matrix - Adjacency matrix including self-connections. Shape: [batch_size, num_nodes, num_nodes]\n",
    "            print_attn_probs - If True, the attention weights are printed during the forward pass (for debugging purposes)\n",
    "        \"\"\"\n",
    "        # Things needs to be done to get the original adjacent matrix in full. This is not a good practice (from saving\n",
    "        # memory perspective), but we will do it just to see how things work\n",
    "        adj_matrix = utils.to_dense_adj(edge_index)\n",
    "        node_feats = node_feats.view(1, node_feats.shape[0], node_feats.shape[1]) # remove 1st dimension with is for old-way of representation of Batch\n",
    "\n",
    "\n",
    "        batch_size, num_nodes = node_feats.size(0), node_feats.size(1)\n",
    "\n",
    "        # Apply linear layer and sort nodes by head\n",
    "        node_feats = self.projection(node_feats)\n",
    "        node_feats = node_feats.view(batch_size, num_nodes, self.num_heads, -1)\n",
    "\n",
    "        # We need to calculate the attention logits for every edge in the adjacency matrix\n",
    "        # Doing this on all possible combinations of nodes is very expensive\n",
    "        # => Create a tensor of [W*h_i||W*h_j] with i and j being the indices of all edges\n",
    "        edges = adj_matrix.nonzero(as_tuple=False)  # Returns indices where the adjacency matrix is not 0 => edges\n",
    "        node_feats_flat = node_feats.view(batch_size * num_nodes, self.num_heads, -1)\n",
    "        edge_indices_row = edges[:, 0] * num_nodes + edges[:, 1]\n",
    "        edge_indices_col = edges[:, 0] * num_nodes + edges[:, 2]\n",
    "        a_input = torch.cat([\n",
    "            torch.index_select(input=node_feats_flat, index=edge_indices_row, dim=0),\n",
    "            torch.index_select(input=node_feats_flat, index=edge_indices_col, dim=0)\n",
    "        ],\n",
    "            dim=-1)  # Index select returns a tensor with node_feats_flat being indexed at the desired positions along dim=0\n",
    "\n",
    "        # Calculate attention MLP output (independent for each head)\n",
    "        attn_logits = torch.einsum('bhc,hc->bh', a_input, self.a)\n",
    "        attn_logits = self.leakyrelu(attn_logits)\n",
    "\n",
    "        # Map list of attention values back into a matrix\n",
    "        attn_matrix = attn_logits.new_zeros(adj_matrix.shape + (self.num_heads,)).fill_(-9e15)\n",
    "        attn_matrix[adj_matrix[..., None].repeat(1, 1, 1, self.num_heads) == 1] = attn_logits.reshape(-1)\n",
    "\n",
    "        # Weighted average of attention\n",
    "        attn_probs = F.softmax(attn_matrix, dim=2)\n",
    "        if print_attn_probs:\n",
    "            print(\"Attention probs\\n\", attn_probs.permute(0, 3, 1, 2))\n",
    "        node_feats = torch.einsum('bijh,bjhc->bihc', attn_probs, node_feats)\n",
    "\n",
    "        # If heads should be concatenated, we can do this by reshaping. Otherwise, take mean\n",
    "        if self.concat_heads:\n",
    "            node_feats = node_feats.reshape(batch_size, num_nodes, -1)\n",
    "        else:\n",
    "            node_feats = node_feats.mean(dim=2)\n",
    "\n",
    "        return node_feats.squeeze() # remove the 1st dimension since it is for old-batching-style\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d00bc3",
   "metadata": {},
   "source": [
    "#### 3 Graph neural network\n",
    "The GNN applies a sequence of graph layers (GCN, GAT, or GraphConv), ReLU as activation function, and dropout for regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcb3e3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn_layer_by_name = {\n",
    "    \"GCN\": geom_nn.GCNConv,\n",
    "    \"GAT\": geom_nn.GATConv,\n",
    "    \"GraphConv\": geom_nn.GraphConv,\n",
    "    \"myAttention\": GATLayer,\n",
    "    \"myGCN\": GCNLayer,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55d91943",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, c_in, c_hidden, c_out, num_layers=2, layer_name=\"GCN\", dp_rate=0.1, **kwargs):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            c_in - Dimension of input features\n",
    "            c_hidden - Dimension of hidden features\n",
    "            c_out - Dimension of the output features. Usually number of classes in classification\n",
    "            num_layers - Number of \"hidden\" graph layers\n",
    "            layer_name - String of the graph layer to use\n",
    "            dp_rate - Dropout rate to apply throughout the network\n",
    "            kwargs - Additional arguments for the graph layer (e.g. number of heads for GAT)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        gnn_layer = gnn_layer_by_name[layer_name]\n",
    "        \n",
    "        layers = []\n",
    "        in_channels, out_channels = c_in, c_hidden\n",
    "        for l_idx in range(num_layers-1):\n",
    "            layers += [\n",
    "                gnn_layer(in_channels=in_channels, \n",
    "                          out_channels=out_channels,\n",
    "                          **kwargs),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(dp_rate)\n",
    "            ]\n",
    "            in_channels = c_hidden\n",
    "        layers += [gnn_layer(in_channels=in_channels, \n",
    "                             out_channels=c_out,\n",
    "                             **kwargs)]\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            x - Input features per node\n",
    "            edge_index - List of vertex index pairs representing the edges in the graph (PyTorch geometric notation)\n",
    "        \"\"\"\n",
    "        for l in self.layers:\n",
    "            # For graph layers, we need to add the \"edge_index\" tensor as additional input\n",
    "            # All PyTorch Geometric graph layer inherit the class \"MessagePassing\", hence\n",
    "            # we can simply check the class type.\n",
    "            if isinstance(l, geom_nn.MessagePassing) or isinstance(l, GATLayer) or isinstance(l, GCNLayer):\n",
    "                x = l(x, edge_index)\n",
    "            else:\n",
    "                x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e4c635",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d86c5be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphGNNModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, c_in, c_hidden, c_out, dp_rate_linear=0.5, **kwargs):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            c_in - Dimension of input features\n",
    "            c_hidden - Dimension of hidden features\n",
    "            c_out - Dimension of output features (usually number of classes)\n",
    "            dp_rate_linear - Dropout rate before the linear layer (usually much higher than inside the GNN)\n",
    "            kwargs - Additional arguments for the GNNModel object\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.GNN = GNNModel(c_in=c_in, \n",
    "                            c_hidden=c_hidden, \n",
    "                            c_out=c_hidden, # Not our prediction output yet!\n",
    "                            **kwargs)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Dropout(dp_rate_linear),\n",
    "            nn.Linear(c_hidden, c_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, batch_idx):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            x - Input features per node\n",
    "            edge_index - List of vertex index pairs representing the edges in the graph (PyTorch geometric notation)\n",
    "            batch_idx - Index of batch element for each node\n",
    "        \"\"\"\n",
    "        x = self.GNN(x, edge_index)\n",
    "        x = geom_nn.global_mean_pool(x, batch_idx) # Average pooling\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9692a9bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94d4798f",
   "metadata": {},
   "source": [
    "Finally, we can create a PyTorch Lightning module to handle the training. It is similar to the modules we have seen before and does nothing surprising in terms of training. As we have a binary classification task, we use the Binary Cross Entropy loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b00ee97",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d945902",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PyTorch Lightning\n",
    "try:\n",
    "    import pytorch_lightning as pl\n",
    "except ModuleNotFoundError:\n",
    "    !pip install --quiet pytorch-lightning>=1.4\n",
    "    import pytorch_lightning as pl\n",
    "\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61eab15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphLevelGNN(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, **model_kwargs):\n",
    "        super().__init__()\n",
    "        # Saving hyperparameters\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.model = GraphGNNModel(**model_kwargs)\n",
    "        self.loss_module = nn.BCEWithLogitsLoss() if self.hparams.c_out == 1 else nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, data, mode=\"train\"):\n",
    "        x, edge_index, batch_idx = data.x, data.edge_index, data.batch\n",
    "        x = self.model(x, edge_index, batch_idx)\n",
    "        x = x.squeeze(dim=-1)\n",
    "        \n",
    "        if self.hparams.c_out == 1:\n",
    "            preds = (x > 0).float()\n",
    "            data.y = data.y.float()\n",
    "        else:\n",
    "            preds = x.argmax(dim=-1)\n",
    "        loss = self.loss_module(x, data.y)\n",
    "        acc = (preds == data.y).sum().float() / preds.shape[0]\n",
    "        return loss, acc\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=1e-2, weight_decay=0.0) # High lr because of small dataset and small model\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, acc = self.forward(batch, mode=\"train\")\n",
    "        self.log('train_loss', loss)\n",
    "        self.log('train_acc', acc)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        _, acc = self.forward(batch, mode=\"val\")\n",
    "        self.log('val_acc', acc)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        _, acc = self.forward(batch, mode=\"test\")\n",
    "        self.log('test_acc', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e1c6f7",
   "metadata": {},
   "source": [
    "Below we train the model on our dataset. It resembles the typical training functions we have seen so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2264cf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_graph_classifier(model_name, **model_kwargs):\n",
    "    pl.seed_everything(42)\n",
    "    \n",
    "    # Create a PyTorch Lightning trainer with the generation callback\n",
    "    root_dir = os.path.join(CHECKPOINT_PATH, \"GraphLevel\" + model_name)\n",
    "    os.makedirs(root_dir, exist_ok=True)\n",
    "    \n",
    "    trainer = pl.Trainer(default_root_dir=root_dir,\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\")],\n",
    "#                          gpus=1 if str(device).startswith(\"cuda\") else 0,\n",
    "                         devices=1 if torch.cuda.is_available() else None,  # limiting got iPython runs\n",
    "                         max_epochs=500,\n",
    "                         progress_bar_refresh_rate=0)\n",
    "    \n",
    "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, f\"GraphLevel{model_name}.ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(\"Found pretrained model, loading...\")\n",
    "        model = GraphLevelGNN.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        pl.seed_everything(42)\n",
    "        model = GraphLevelGNN(c_in=tu_dataset.num_node_features, \n",
    "                              c_out=1 if tu_dataset.num_classes==2 else tu_dataset.num_classes, \n",
    "                              **model_kwargs)\n",
    "        trainer.fit(model, graph_train_loader, graph_val_loader)\n",
    "        model = GraphLevelGNN.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "        \n",
    "\n",
    "    # Test best model on validation and test set\n",
    "    train_result = trainer.test(model, graph_train_loader, verbose=False)\n",
    "    test_result = trainer.test(model, graph_test_loader, verbose=False)\n",
    "    result = {\"test\": test_result[0]['test_acc'], \"train\": train_result[0]['test_acc']} \n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a0e89e",
   "metadata": {},
   "source": [
    "Finally, let's perform the training and testing. Feel free to experiment with different GNN layers, hyperparameters, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09400a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "c:\\users\\bahramih\\desktop\\seq2seq\\graph\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "\n",
      "  | Name        | Type              | Params\n",
      "--------------------------------------------------\n",
      "0 | model       | GraphGNNModel     | 266 K \n",
      "1 | loss_module | BCEWithLogitsLoss | 0     \n",
      "--------------------------------------------------\n",
      "266 K     Trainable params\n",
      "0         Non-trainable params\n",
      "266 K     Total params\n",
      "1.067     Total estimated model params size (MB)\n",
      "c:\\users\\bahramih\\desktop\\seq2seq\\graph\\venv\\lib\\site-packages\\pytorch_lightning\\utilities\\data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n",
      "c:\\users\\bahramih\\desktop\\seq2seq\\graph\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1933: PossibleUserWarning: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "c:\\users\\bahramih\\desktop\\seq2seq\\graph\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:495: PossibleUserWarning: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "model, result = train_graph_classifier(model_name=\"GraphConv\", \n",
    "                                       c_hidden=256, \n",
    "                                       layer_name=\"GraphConv\", \n",
    "                                       num_layers=3, \n",
    "                                       dp_rate_linear=0.5,\n",
    "                                       dp_rate=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8a5ba9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train performance: 95.83%\n",
      "Test performance:  89.47%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train performance: {100.0*result['train']:4.2f}%\")\n",
    "print(f\"Test performance:  {100.0*result['test']:4.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78c9553",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9488be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e502d5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bcaf19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078a6fc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8d57b27",
   "metadata": {},
   "source": [
    "# Code for PyCharm\n",
    "\n",
    "The following is for when I wanted to run the above code all together in a PyCharm IDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619133d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# import torch\n",
    "# from torch import nn\n",
    "# import torch.nn.functional as F\n",
    "# import torch.utils.data as data\n",
    "# import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "# import torch_geometric\n",
    "# import torch_geometric.nn as geom_nn\n",
    "# import torch_geometric.loader as loader\n",
    "# # ***********************************************\n",
    "# from torch_geometric import utils # to convert the edge index to actual adjacent matrix for some preliminary implementations (not a good practice)\n",
    "\n",
    "# from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "\n",
    "# import pytorch_lightning as pl\n",
    "# # Setting the seed\n",
    "# pl.seed_everything(42)\n",
    "\n",
    "\n",
    "# # Path to the folder where the pretrained models are saved\n",
    "# CHECKPOINT_PATH = \"./saved_models/test\"\n",
    "\n",
    "# # Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "# DATASET_PATH = \"./data\"\n",
    "\n",
    "# num_workers = 6 # number of cpus to emply (for data loader function)\n",
    "\n",
    "# device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# print(device)\n",
    "\n",
    "# # Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# # ***********************************************\n",
    "\n",
    "# # These are defined by myself for testing, and different that the internal classes defined in torch-geometric in:\n",
    "# # 1) We use the actual adjacent matrix, rather than the edge-index. from memory saving practice this is not good.\n",
    "# # But we do it for learning purposes\n",
    "# class GCNLayer(nn.Module):\n",
    "\n",
    "#     def __init__(self, in_channels, out_channels):\n",
    "#         super().__init__()\n",
    "#         self.projection = nn.Linear(in_channels, out_channels)\n",
    "\n",
    "#     def forward(self, node_feats, edge_index):\n",
    "#         \"\"\"\n",
    "#         Inputs:\n",
    "#             node_feats - Tensor with node features of shape [batch_size, num_nodes, c_in]\n",
    "#             adj_matrix - Batch of adjacency matrices of the graph. If there is an edge from i to j, adj_matrix[b,i,j]=1 else 0.\n",
    "#                          Supports directed edges by non-symmetric matrices. Assumes to already have added the identity connections. \n",
    "#                          Shape: [batch_size, num_nodes, num_nodes]\n",
    "#         \"\"\"\n",
    "#         # Num neighbours = number of incoming edges\n",
    "#         adj_matrix = utils.to_dense_adj(edge_index)\n",
    "#         node_feats = node_feats.view(1, node_feats.shape[0], node_feats.shape[1]) # remove 1st dimension with is for old-way of representation of Batch\n",
    "#         num_neighbours = adj_matrix.sum(dim=-1, keepdims=True)\n",
    "#         node_feats = self.projection(node_feats)\n",
    "#         node_feats = torch.bmm(adj_matrix, node_feats)\n",
    "#         node_feats = node_feats / num_neighbours\n",
    "#         return node_feats.squeeze() # remove the 1st dimension since it is for old-batching-style\n",
    "\n",
    "# class GATLayer(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels, num_heads=1, concat_heads=True, alpha=0.2):\n",
    "#         \"\"\"\n",
    "#         Inputs:\n",
    "#             in_channels - Dimensionality of input features\n",
    "#             out_channels - Dimensionality of output features\n",
    "#             num_heads - Number of heads, i.e. attention mechanisms to apply in parallel. The\n",
    "#                         output features are equally split up over the heads if concat_heads=True.\n",
    "#             concat_heads - If True, the output of the different heads is concatenated instead of averaged.\n",
    "#             alpha - Negative slope of the LeakyReLU activation.\n",
    "#         \"\"\"\n",
    "#         super().__init__()\n",
    "#         self.num_heads = num_heads\n",
    "#         self.concat_heads = concat_heads\n",
    "#         if self.concat_heads:\n",
    "#             assert out_channels % num_heads == 0, \"Number of output features must be a multiple of the count of heads.\"\n",
    "#             out_channels = out_channels // num_heads\n",
    "\n",
    "#         # Sub-modules and parameters needed in the layer\n",
    "#         self.projection = nn.Linear(in_channels, out_channels * num_heads)\n",
    "#         self.a = nn.Parameter(torch.Tensor(num_heads, 2 * out_channels))  # One per head\n",
    "#         self.leakyrelu = nn.LeakyReLU(alpha)\n",
    "\n",
    "#         # Initialization from the original implementation\n",
    "#         nn.init.xavier_uniform_(self.projection.weight.data, gain=1.414)\n",
    "#         nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "#     def forward(self, node_feats, edge_index, print_attn_probs=False):\n",
    "#         \"\"\"\n",
    "#         Inputs:\n",
    "#             node_feats - Input features of the node. Shape: [batch_size, c_in]\n",
    "#             adj_matrix - Adjacency matrix including self-connections. Shape: [batch_size, num_nodes, num_nodes]\n",
    "#             print_attn_probs - If True, the attention weights are printed during the forward pass (for debugging purposes)\n",
    "#         \"\"\"\n",
    "#         # Things needs to be done to get the original adjacent matrix in full. This is not a good practice (from saving\n",
    "#         # memory perspective), but we will do it just to see how things work\n",
    "#         adj_matrix = utils.to_dense_adj(edge_index)\n",
    "#         node_feats = node_feats.view(1, node_feats.shape[0], node_feats.shape[1]) # remove 1st dimension with is for old-way of representation of Batch\n",
    "\n",
    "\n",
    "#         batch_size, num_nodes = node_feats.size(0), node_feats.size(1)\n",
    "\n",
    "#         # Apply linear layer and sort nodes by head\n",
    "#         node_feats = self.projection(node_feats)\n",
    "#         node_feats = node_feats.view(batch_size, num_nodes, self.num_heads, -1)\n",
    "\n",
    "#         # We need to calculate the attention logits for every edge in the adjacency matrix\n",
    "#         # Doing this on all possible combinations of nodes is very expensive\n",
    "#         # => Create a tensor of [W*h_i||W*h_j] with i and j being the indices of all edges\n",
    "#         edges = adj_matrix.nonzero(as_tuple=False)  # Returns indices where the adjacency matrix is not 0 => edges\n",
    "#         node_feats_flat = node_feats.view(batch_size * num_nodes, self.num_heads, -1)\n",
    "#         edge_indices_row = edges[:, 0] * num_nodes + edges[:, 1]\n",
    "#         edge_indices_col = edges[:, 0] * num_nodes + edges[:, 2]\n",
    "#         a_input = torch.cat([\n",
    "#             torch.index_select(input=node_feats_flat, index=edge_indices_row, dim=0),\n",
    "#             torch.index_select(input=node_feats_flat, index=edge_indices_col, dim=0)\n",
    "#         ],\n",
    "#             dim=-1)  # Index select returns a tensor with node_feats_flat being indexed at the desired positions along dim=0\n",
    "\n",
    "#         # Calculate attention MLP output (independent for each head)\n",
    "#         attn_logits = torch.einsum('bhc,hc->bh', a_input, self.a)\n",
    "#         attn_logits = self.leakyrelu(attn_logits)\n",
    "\n",
    "#         # Map list of attention values back into a matrix\n",
    "#         attn_matrix = attn_logits.new_zeros(adj_matrix.shape + (self.num_heads,)).fill_(-9e15)\n",
    "#         attn_matrix[adj_matrix[..., None].repeat(1, 1, 1, self.num_heads) == 1] = attn_logits.reshape(-1)\n",
    "\n",
    "#         # Weighted average of attention\n",
    "#         attn_probs = F.softmax(attn_matrix, dim=2)\n",
    "#         if print_attn_probs:\n",
    "#             print(\"Attention probs\\n\", attn_probs.permute(0, 3, 1, 2))\n",
    "#         node_feats = torch.einsum('bijh,bjhc->bihc', attn_probs, node_feats)\n",
    "\n",
    "#         # If heads should be concatenated, we can do this by reshaping. Otherwise, take mean\n",
    "#         if self.concat_heads:\n",
    "#             node_feats = node_feats.reshape(batch_size, num_nodes, -1)\n",
    "#         else:\n",
    "#             node_feats = node_feats.mean(dim=2)\n",
    "\n",
    "#         return node_feats.squeeze() # remove the 1st dimension since it is for old-batching-style\n",
    "\n",
    "# # ***********************************************\n",
    "# gnn_layer_by_name = {\n",
    "#     \"GCN\": geom_nn.GCNConv,\n",
    "#     \"GAT\": geom_nn.GATConv,\n",
    "#     \"GraphConv\": geom_nn.GraphConv,\n",
    "#     \"myAttention\": GATLayer,\n",
    "#     \"myGCN\": GCNLayer,\n",
    "# }\n",
    "\n",
    "# class GNNModel(nn.Module):\n",
    "\n",
    "#     def __init__(self, c_in, c_hidden, c_out, num_layers=2, layer_name=\"GCN\", dp_rate=0.1, **kwargs):\n",
    "#         \"\"\"\n",
    "#         Inputs:\n",
    "#             c_in - Dimension of input features\n",
    "#             c_hidden - Dimension of hidden features\n",
    "#             c_out - Dimension of the output features. Usually number of classes in classification\n",
    "#             num_layers - Number of \"hidden\" graph layers\n",
    "#             layer_name - String of the graph layer to use\n",
    "#             dp_rate - Dropout rate to apply throughout the network\n",
    "#             kwargs - Additional arguments for the graph layer (e.g. number of heads for GAT)\n",
    "#         \"\"\"\n",
    "#         super().__init__()\n",
    "#         gnn_layer = gnn_layer_by_name[layer_name]\n",
    "\n",
    "#         layers = []\n",
    "#         in_channels, out_channels = c_in, c_hidden\n",
    "#         for l_idx in range(num_layers - 1):\n",
    "#             layers += [\n",
    "#                 gnn_layer(in_channels=in_channels,\n",
    "#                           out_channels=out_channels,\n",
    "#                           **kwargs),\n",
    "#                 nn.ReLU(inplace=True),\n",
    "#                 nn.Dropout(dp_rate)\n",
    "#             ]\n",
    "#             in_channels = c_hidden\n",
    "#         layers += [gnn_layer(in_channels=in_channels,\n",
    "#                              out_channels=c_out,\n",
    "#                              **kwargs)]\n",
    "#         self.layers = nn.ModuleList(layers)\n",
    "\n",
    "#     def forward(self, x, edge_index):\n",
    "#         \"\"\"\n",
    "#         Inputs:\n",
    "#             x - Input features per node\n",
    "#             edge_index - List of vertex index pairs representing the edges in the graph (PyTorch geometric notation)\n",
    "#         \"\"\"\n",
    "#         for l in self.layers:\n",
    "#             # For graph layers, we need to add the \"edge_index\" tensor as additional input\n",
    "#             # All PyTorch Geometric graph layer inherit the class \"MessagePassing\", hence\n",
    "#             # we can simply check the class type.\n",
    "#             # ***********************************************\n",
    "#             if isinstance(l, geom_nn.MessagePassing) or isinstance(l, GATLayer) or isinstance(l, GCNLayer):\n",
    "#                 x = l(x, edge_index)\n",
    "#             else:\n",
    "#                 x = l(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "# class GraphGNNModel(nn.Module):\n",
    "\n",
    "#     def __init__(self, c_in, c_hidden, c_out, dp_rate_linear=0.5, **kwargs):\n",
    "#         \"\"\"\n",
    "#         Inputs:\n",
    "#             c_in - Dimension of input features\n",
    "#             c_hidden - Dimension of hidden features\n",
    "#             c_out - Dimension of output features (usually number of classes)\n",
    "#             dp_rate_linear - Dropout rate before the linear layer (usually much higher than inside the GNN)\n",
    "#             kwargs - Additional arguments for the GNNModel object\n",
    "#         \"\"\"\n",
    "#         super().__init__()\n",
    "#         self.GNN = GNNModel(c_in=c_in,\n",
    "#                             c_hidden=c_hidden,\n",
    "#                             c_out=c_hidden,  # Not our prediction output yet!\n",
    "#                             **kwargs)\n",
    "#         self.head = nn.Sequential(\n",
    "#             nn.Dropout(dp_rate_linear),\n",
    "#             nn.Linear(c_hidden, c_out)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x, edge_index, batch_idx):\n",
    "#         \"\"\"\n",
    "#         Inputs:\n",
    "#             x - Input features per node\n",
    "#             edge_index - List of vertex index pairs representing the edges in the graph (PyTorch geometric notation)\n",
    "#             batch_idx - Index of batch element for each node\n",
    "#         \"\"\"\n",
    "#         x = self.GNN(x, edge_index)\n",
    "#         x = geom_nn.global_mean_pool(x, batch_idx)  # Average pooling\n",
    "#         x = self.head(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "\n",
    "# class GraphLevelGNN(pl.LightningModule):\n",
    "\n",
    "#     def __init__(self, **model_kwargs):\n",
    "#         super().__init__()\n",
    "#         # Saving hyperparameters\n",
    "#         self.save_hyperparameters()\n",
    "\n",
    "#         self.model = GraphGNNModel(**model_kwargs)\n",
    "#         self.loss_module = nn.BCEWithLogitsLoss() if self.hparams.c_out == 1 else nn.CrossEntropyLoss()\n",
    "\n",
    "#     def forward(self, data, mode=\"train\"):\n",
    "#         x, edge_index, batch_idx = data.x, data.edge_index, data.batch\n",
    "#         x = self.model(x, edge_index, batch_idx)\n",
    "#         x = x.squeeze(dim=-1)\n",
    "\n",
    "#         if self.hparams.c_out == 1:\n",
    "#             preds = (x > 0).float()\n",
    "#             data.y = data.y.float()\n",
    "#         else:\n",
    "#             preds = x.argmax(dim=-1)\n",
    "#         loss = self.loss_module(x, data.y)\n",
    "#         acc = (preds == data.y).sum().float() / preds.shape[0]\n",
    "#         return loss, acc\n",
    "\n",
    "#     def configure_optimizers(self):\n",
    "#         optimizer = optim.AdamW(self.parameters(), lr=1e-2,\n",
    "#                                 weight_decay=0.0)  # High lr because of small dataset and small model\n",
    "#         return optimizer\n",
    "\n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         loss, acc = self.forward(batch, mode=\"train\")\n",
    "#         self.log('train_loss', loss)\n",
    "#         self.log('train_acc', acc)\n",
    "#         return loss\n",
    "\n",
    "#     def validation_step(self, batch, batch_idx):\n",
    "#         _, acc = self.forward(batch, mode=\"val\")\n",
    "#         self.log('val_acc', acc)\n",
    "\n",
    "#     def test_step(self, batch, batch_idx):\n",
    "#         _, acc = self.forward(batch, mode=\"test\")\n",
    "#         self.log('test_acc', acc)\n",
    "\n",
    "\n",
    "# def perform_steps():\n",
    "#     tu_dataset = torch_geometric.datasets.TUDataset(root=DATASET_PATH, name=\"MUTAG\")\n",
    "\n",
    "#     print(\"Data object:\", tu_dataset.data)\n",
    "#     print(\"Length:\", len(tu_dataset))\n",
    "#     print(f\"Average label: {tu_dataset.data.y.float().mean().item():4.2f}\")\n",
    "\n",
    "#     torch.manual_seed(42)\n",
    "#     tu_dataset.shuffle()\n",
    "#     train_dataset = tu_dataset[:150]\n",
    "#     test_dataset = tu_dataset[150:]\n",
    "\n",
    "#     graph_train_loader = loader.DataLoader(train_dataset, batch_size=64, num_workers=num_workers, shuffle=True)\n",
    "#     graph_val_loader = loader.DataLoader(test_dataset, batch_size=64, num_workers=num_workers) # Additional loader if you want to change to a larger dataset\n",
    "#     graph_test_loader = loader.DataLoader(test_dataset, batch_size=64, num_workers=num_workers)\n",
    "\n",
    "#     batch = next(iter(graph_test_loader))\n",
    "#     print(\"Batch:\", batch)\n",
    "#     print(\"Labels:\", batch.y[:10])\n",
    "#     print(\"Batch indices:\", batch.batch[:40])\n",
    "\n",
    "#     def train_graph_classifier(model_name, **model_kwargs):\n",
    "#         pl.seed_everything(42)\n",
    "\n",
    "#         # Create a PyTorch Lightning trainer with the generation callback\n",
    "#         root_dir = os.path.join(CHECKPOINT_PATH, \"GraphLevel\" + model_name)\n",
    "#         os.makedirs(root_dir, exist_ok=True)\n",
    "\n",
    "#         trainer = pl.Trainer(default_root_dir=root_dir,\n",
    "#                              callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\")],\n",
    "#                              #                          gpus=1 if str(device).startswith(\"cuda\") else 0,\n",
    "#                              devices=1 if torch.cuda.is_available() else None,  # limiting got iPython runs\n",
    "#                              max_epochs=500,\n",
    "#                              progress_bar_refresh_rate=0)\n",
    "\n",
    "#         trainer.logger._default_hp_metric = None  # Optional logging argument that we don't need\n",
    "\n",
    "#         # Check whether pretrained model exists. If yes, load it and skip training\n",
    "#         pretrained_filename = os.path.join(CHECKPOINT_PATH, f\"GraphLevel{model_name}.ckpt\")\n",
    "#         if os.path.isfile(pretrained_filename):\n",
    "#             print(\"Found pretrained model, loading...\")\n",
    "#             model = GraphLevelGNN.load_from_checkpoint(pretrained_filename)\n",
    "#         else:\n",
    "#             pl.seed_everything(42)\n",
    "#             model = GraphLevelGNN(c_in=tu_dataset.num_node_features,\n",
    "#                                   c_out=1 if tu_dataset.num_classes == 2 else tu_dataset.num_classes,\n",
    "#                                   **model_kwargs)\n",
    "#             trainer.fit(model, graph_train_loader, graph_val_loader)\n",
    "#             model = GraphLevelGNN.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "\n",
    "#         # Test best model on validation and test set\n",
    "#         train_result = trainer.test(model, graph_train_loader, verbose=False)\n",
    "#         test_result = trainer.test(model, graph_test_loader, verbose=False)\n",
    "#         result = {\"test\": test_result[0]['test_acc'], \"train\": train_result[0]['test_acc']}\n",
    "#         return model, result\n",
    "\n",
    "\n",
    "#     model, result = train_graph_classifier(model_name=\"myAttention\",\n",
    "#                                            c_hidden=256,\n",
    "#                                            layer_name=\"myAttention\",\n",
    "#                                            num_layers=3,\n",
    "#                                            dp_rate_linear=0.5,\n",
    "#                                            dp_rate=0.0)\n",
    "\n",
    "#     # model, result = train_graph_classifier(model_name=\"GraphConv\",\n",
    "#     #                                        c_hidden=256,\n",
    "#     #                                        layer_name=\"GraphConv\",\n",
    "#     #                                        num_layers=3,\n",
    "#     #                                        dp_rate_linear=0.5,\n",
    "#     #                                        dp_rate=0.0)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     perform_steps()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
