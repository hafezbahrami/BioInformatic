{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2c19ff4",
   "metadata": {},
   "source": [
    "# Graph classification vs Node classification\n",
    "\n",
    "- Graph classification is very similar to a general image classification in CNN. Graph classification refers to the problem of classifiying entire graphs (in contrast to nodes), given a dataset of graphs, based on some structural graph properties. \n",
    "- In node classification, we use embedding vector for each node to come up with x_feature for each node. Here, for graph classification, we want to embed entire graphs, and we want to embed those graphs in such a way so that they are linearly separable given a task at hand.\n",
    "\n",
    "\n",
    "<img src=\"./img/graph_classification.png\" height=\"400\" width=\"400\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f29ceab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.datasets import TUDataset\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef66f6d",
   "metadata": {},
   "source": [
    "# AIDS dataset graph\n",
    "\n",
    "- As you will see below, we will have 2000 graph, each graph is represented by XX number of nodes (different nodes in each graph, similar to NLP where we have sentences with different lengths and we used padding to make all sentences at the same length). But each node is represented by an embedding of size 38 (similar to NLP where each word was embedded). Each graph has an attribute indicating if the compound is active or inactive against HIV (basically the two classes we want to figure out).\n",
    "\n",
    "- There are two classes for all these graphs: positive and negative AIDS molecular structure\n",
    "- TUDataset library from Pytorch Geometric: A collection of benchmark datasets for learning with graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b50e798",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = \"cpu\"\n",
    "\n",
    "dataset_name = \"AIDS\"\n",
    "dataset = TUDataset(\".\", name=dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efbc582f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: AIDS(2000)\n",
      "====================\n",
      "Number of graphs: 2000\n",
      "Size of x_features: 38\n",
      "Number of classes: 2\n",
      "one sample out of 2000 AIDS graph data:  Data(edge_index=[2, 106], x=[47, 38], edge_attr=[106, 3], y=[1])\n",
      "one sample out of 2000 AIDS graph data:  Data(edge_index=[2, 22], x=[11, 38], edge_attr=[22, 3], y=[1])\n",
      "\n",
      "\n",
      "=============================================================\n",
      "LET's EXPLORE ONE OF THESE 2000 GRAPHS\n",
      "\n",
      "Data(edge_index=[2, 106], x=[47, 38], edge_attr=[106, 3], y=[1])\n",
      "-------------------------------------------------------------\n",
      "Number of nodes: 47\n",
      "Number of edges: 106\n",
      "Average node degree: 2.26\n",
      "number of features (size of embedding vector ):  38\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Is undirected: True\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print('Dataset: {}'.format(dataset))\n",
    "print('====================')\n",
    "print('Number of graphs: {}'.format(len(dataset)))\n",
    "print('Size of x_features: {}'.format(dataset.num_features))\n",
    "print('Number of classes: {}'.format(dataset.num_classes))\n",
    "\n",
    "print(\"one sample out of 2000 AIDS graph data: \", dataset[0])\n",
    "print(\"one sample out of 2000 AIDS graph data: \", dataset[1])\n",
    "\n",
    "print(\"\\n\")\n",
    "print('=============================================================')\n",
    "print('LET\\'s EXPLORE ONE OF THESE 2000 GRAPHS')\n",
    "\n",
    "\n",
    "data = dataset[0]\n",
    "print()\n",
    "print(data)\n",
    "print('-------------------------------------------------------------')\n",
    "\n",
    "# Gather some statistics about the first graph.\n",
    "print('Number of nodes: {}'.format(data.num_nodes))\n",
    "print('Number of edges: {}'.format(data.num_edges))\n",
    "print('Average node degree: {:.2f}'.format(data.num_edges / data.num_nodes))\n",
    "print(\"number of features (size of embedding vector ): \",data.num_features)\n",
    "print('Has isolated nodes: {}'.format(data.has_isolated_nodes()))\n",
    "print('Has self-loops: {}'.format(data.has_self_loops()))\n",
    "print('Is undirected: {}'.format(data.is_undirected()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae98cf6",
   "metadata": {},
   "source": [
    "As seen above, this dataset provides 2000 different graphs, and the task is to classify each graph into one out of two classes.\n",
    "\n",
    "By inspecting the first graph object of the dataset, we can see that it comes with 47 nodes (with 38-dimensional feature vectors) and 106 edges (leading to an average node degree of 2.26). It also comes with exactly one graph label (y=[1]), and, in addition to previous datasets, provides addtional 3-dimensional edge features (edge_attr=[106, 3]). However, for the sake of simplicity, we will not make use of those x_feature and only focus on x_feature comes from node embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e95b55",
   "metadata": {},
   "source": [
    "**PyTorch Geometric** provides some useful utilities for working with graph datasets, e.g., we can shuffle the dataset and use the first 1500 graphs as training graphs, while using the remaining ones for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7562313d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs: 1500\n",
      "Number of test graphs: 500\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(12345)\n",
    "dataset = dataset.shuffle()\n",
    "\n",
    "train_dataset = dataset[:1500]\n",
    "test_dataset = dataset[1500:]\n",
    "\n",
    "print('Number of training graphs: {}'.format(len(train_dataset)) )\n",
    "print('Number of test graphs: {}'.format(len(test_dataset)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c380bf34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fcd49d64",
   "metadata": {},
   "source": [
    "# Mini-batching of graphs (DataLoader Class)\n",
    "\n",
    "Since graphs in graph classification datasets are usually small, a good idea is to **batch the graphs** before inputting them into a Graph Neural Network to guarantee full GPU utilization.\n",
    "In the image or language domain, this procedure is typically achieved by **rescaling** or **padding** each example into a set of equally-sized shapes, and examples are then grouped in an additional dimension.\n",
    "The length of this dimension is then equal to the number of examples grouped in a mini-batch and is typically referred to as the `batch_size`.\n",
    "\n",
    "However, for GNNs the two approaches described above are either not feasible or may result in a lot of unnecessary memory consumption.\n",
    "Therefore, PyTorch Geometric opts for another approach to achieve parallelization across a number of examples. Here, adjacency matrices are stacked in a diagonal fashion (creating a giant graph that holds multiple isolated subgraphs), and node and target features are simply concatenated in the node dimension:\n",
    "\n",
    "<img src=\"./img/graph_mini_batching.png\" height=\"400\" width=\"400\"/>\n",
    "\n",
    "\n",
    "This procedure has some crucial advantages over other batching procedures:\n",
    "\n",
    "GNN operators that rely on a message passing scheme do not need to be modified since messages are not exchanged between two nodes that belong to different graphs.\n",
    "\n",
    "There is no computational or memory overhead since adjacency matrices are saved in a sparse fashion holding only non-zero entries, i.e., the edges.\n",
    "\n",
    "PyTorch Geometric automatically takes care of batching multiple graphs into a single giant graph with the help of the torch_geometric.data.DataLoader class.\n",
    "\n",
    "\n",
    "Below, for test dataset, we opt for a batch_size of 64, leading to 8 (randomly shuffled) mini-batches, containing all  2⋅64+22=150  graphs. Furthermore, each Batch object is equipped with a batch vector, which maps each node to its respective graph in the batch:\n",
    "\n",
    "$$batch = [0, ...,0,1, ..., 1, 2]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8e44bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(edge_index=[2, 2024], x=[993, 38], edge_attr=[2024, 3], y=[64], batch=[993], ptr=[65])\n",
      "\n",
      "Step 2\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(edge_index=[2, 2802], x=[1343, 38], edge_attr=[2802, 3], y=[64], batch=[1343], ptr=[65])\n",
      "\n",
      "Step 3\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(edge_index=[2, 2022], x=[968, 38], edge_attr=[2022, 3], y=[64], batch=[968], ptr=[65])\n",
      "\n",
      "Step 4\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(edge_index=[2, 1890], x=[908, 38], edge_attr=[1890, 3], y=[64], batch=[908], ptr=[65])\n",
      "\n",
      "Step 5\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(edge_index=[2, 1918], x=[945, 38], edge_attr=[1918, 3], y=[64], batch=[945], ptr=[65])\n",
      "\n",
      "Step 6\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(edge_index=[2, 1950], x=[950, 38], edge_attr=[1950, 3], y=[64], batch=[950], ptr=[65])\n",
      "\n",
      "Step 7\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(edge_index=[2, 2252], x=[1083, 38], edge_attr=[2252, 3], y=[64], batch=[1083], ptr=[65])\n",
      "\n",
      "Step 8\n",
      "=======\n",
      "Number of graphs in the current batch: 52\n",
      "DataBatch(edge_index=[2, 1776], x=[855, 38], edge_attr=[1776, 3], y=[52], batch=[855], ptr=[53])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "for step, data in enumerate(test_loader):\n",
    "    print('Step {}'.format(step + 1))\n",
    "    print('=======')\n",
    "    print('Number of graphs in the current batch: {}'.format(data.num_graphs) )\n",
    "    print(data)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fd1097",
   "metadata": {},
   "source": [
    "If we look at the report above and focus on the 1st batch, we will see that 64 graphs are bacthed together as if they are in one big graph (that all 64 sub-graphs dis-connected). The fact the size of x if [993, 38], the 993 shows number of total nodes in all these 64 dis-connected graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1177ed93",
   "metadata": {},
   "source": [
    "# Training a Graph Neural Network (GNN)\n",
    "\n",
    "Training a GNN for graph classification usually follows a simple recipe:\n",
    "\n",
    "    - Embed each node by performing multiple rounds of message passing\n",
    "    - Aggregate node embeddings into a unified graph embedding (readout layer)\n",
    "    - Train a final classifier on the graph embedding\n",
    "    \n",
    "There exists multiple **message passing** in literature, but the most common one and simplest is to simply take the average of all neighboring node and the node itself (thier embeddings):\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_{\\mathcal{G}} = \\frac{1}{|\\mathcal{V}|} \\sum_{v \\in \\mathcal{V}} \\mathcal{x}^{(L)}_v\n",
    "$$\n",
    "\n",
    "Above, basically is concat all the embedding vectors of neighbouring nodes and take the average of them. It is common practice for **GCNConv**. Alternatively, below, we could use **GraphConv** where \n",
    "\n",
    "$$\n",
    "\\mathbf{x}_v^{(\\ell+1)} = \\mathbf{W}^{(\\ell + 1)}_1 \\mathbf{x}_v^{(\\ell)} + \\mathbf{W}^{(\\ell + 1)}_2 \\sum_{w \\in \\mathcal{N}(v)} \\mathbf{x}_w^{(\\ell)}\n",
    "$$\n",
    "\n",
    "Where we do not simply average the node iteself and all the neighbors. We add up all the neighbors (with some Weight factor) and then add up that vector with the node embedding.\n",
    "\n",
    "PyTorch Geometric provides this functionality via [`torch_geometric.nn.global_mean_pool`](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.glob.global_mean_pool), which takes in the node embeddings of all nodes in the mini-batch and the assignment vector `batch` to compute a graph embedding of size `[batch_size, hidden_channels]` for each graph in the batch.\n",
    "\n",
    "The final architecture for applying GNNs to the task of graph classification then looks as follows and allows for complete end-to-end training.\n",
    "\n",
    "Below, we make use of the GCNConv with  ReLU(x)=max(x,0)  activation for obtaining localized node embeddings, before we apply our final classifier on top of a graph readout layer.\n",
    "\n",
    "Below shows how the GCNConv does the message passing. Ir inherites from The “MessagePassing” Base Class:\n",
    "https://pytorch-geometric.readthedocs.io/en/latest/notes/create_gnn.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7620b8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(38, 64)\n",
      "  (conv2): GCNConv(64, 64)\n",
      "  (conv3): GCNConv(64, 64)\n",
      "  (lin): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "feature_size = dataset.num_node_features # here in this exampe x_feature is size of 38\n",
    "num_classes = dataset.num_classes\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(feature_size, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = GCN(hidden_channels=64)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4f82cd",
   "metadata": {},
   "source": [
    "#### Traning step\n",
    "Let's train our network for a few epochs to see how well it performs on the training as well as test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db76e1c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Acc: 0.7980, Test Acc: 0.8060\n",
      "Epoch: 002, Train Acc: 0.7980, Test Acc: 0.8060\n",
      "Epoch: 003, Train Acc: 0.7980, Test Acc: 0.8060\n",
      "Epoch: 004, Train Acc: 0.7980, Test Acc: 0.8060\n",
      "Epoch: 005, Train Acc: 0.7953, Test Acc: 0.8020\n",
      "Epoch: 006, Train Acc: 0.7987, Test Acc: 0.8040\n",
      "Epoch: 007, Train Acc: 0.7907, Test Acc: 0.7900\n",
      "Epoch: 008, Train Acc: 0.7980, Test Acc: 0.8040\n",
      "Epoch: 009, Train Acc: 0.7900, Test Acc: 0.7580\n",
      "Epoch: 010, Train Acc: 0.7787, Test Acc: 0.7680\n",
      "Epoch: 011, Train Acc: 0.7967, Test Acc: 0.7920\n",
      "Epoch: 012, Train Acc: 0.7967, Test Acc: 0.7920\n",
      "Epoch: 013, Train Acc: 0.7987, Test Acc: 0.7860\n",
      "Epoch: 014, Train Acc: 0.8047, Test Acc: 0.7820\n",
      "Epoch: 015, Train Acc: 0.7920, Test Acc: 0.7880\n",
      "Epoch: 016, Train Acc: 0.8120, Test Acc: 0.7940\n",
      "Epoch: 017, Train Acc: 0.7953, Test Acc: 0.7620\n",
      "Epoch: 018, Train Acc: 0.8233, Test Acc: 0.7920\n",
      "Epoch: 019, Train Acc: 0.8027, Test Acc: 0.7900\n",
      "Epoch: 020, Train Acc: 0.8233, Test Acc: 0.7940\n",
      "Epoch: 021, Train Acc: 0.8267, Test Acc: 0.8040\n",
      "Epoch: 022, Train Acc: 0.8160, Test Acc: 0.7920\n",
      "Epoch: 023, Train Acc: 0.7960, Test Acc: 0.7840\n",
      "Epoch: 024, Train Acc: 0.8133, Test Acc: 0.7840\n",
      "Epoch: 025, Train Acc: 0.8360, Test Acc: 0.8180\n",
      "Epoch: 026, Train Acc: 0.8240, Test Acc: 0.7840\n",
      "Epoch: 027, Train Acc: 0.8253, Test Acc: 0.7900\n",
      "Epoch: 028, Train Acc: 0.8353, Test Acc: 0.8120\n",
      "Epoch: 029, Train Acc: 0.8107, Test Acc: 0.8080\n",
      "Epoch: 030, Train Acc: 0.8380, Test Acc: 0.8040\n",
      "Epoch: 031, Train Acc: 0.8493, Test Acc: 0.8220\n",
      "Epoch: 032, Train Acc: 0.8273, Test Acc: 0.8180\n",
      "Epoch: 033, Train Acc: 0.8313, Test Acc: 0.8280\n",
      "Epoch: 034, Train Acc: 0.8060, Test Acc: 0.7820\n",
      "Epoch: 035, Train Acc: 0.8433, Test Acc: 0.8240\n",
      "Epoch: 036, Train Acc: 0.8287, Test Acc: 0.8140\n",
      "Epoch: 037, Train Acc: 0.8460, Test Acc: 0.8280\n",
      "Epoch: 038, Train Acc: 0.8487, Test Acc: 0.8340\n",
      "Epoch: 039, Train Acc: 0.8500, Test Acc: 0.8240\n",
      "Epoch: 040, Train Acc: 0.8560, Test Acc: 0.8500\n",
      "Epoch: 041, Train Acc: 0.8540, Test Acc: 0.8280\n",
      "Epoch: 042, Train Acc: 0.8593, Test Acc: 0.8280\n",
      "Epoch: 043, Train Acc: 0.8467, Test Acc: 0.8300\n",
      "Epoch: 044, Train Acc: 0.8520, Test Acc: 0.8440\n",
      "Epoch: 045, Train Acc: 0.8660, Test Acc: 0.8240\n",
      "Epoch: 046, Train Acc: 0.8447, Test Acc: 0.8260\n",
      "Epoch: 047, Train Acc: 0.8607, Test Acc: 0.8260\n",
      "Epoch: 048, Train Acc: 0.8587, Test Acc: 0.8360\n",
      "Epoch: 049, Train Acc: 0.8347, Test Acc: 0.8280\n",
      "Epoch: 050, Train Acc: 0.8540, Test Acc: 0.8360\n",
      "Epoch: 051, Train Acc: 0.8680, Test Acc: 0.8340\n",
      "Epoch: 052, Train Acc: 0.8500, Test Acc: 0.8440\n",
      "Epoch: 053, Train Acc: 0.8607, Test Acc: 0.8460\n",
      "Epoch: 054, Train Acc: 0.8660, Test Acc: 0.8280\n",
      "Epoch: 055, Train Acc: 0.8600, Test Acc: 0.8400\n",
      "Epoch: 056, Train Acc: 0.8627, Test Acc: 0.8280\n",
      "Epoch: 057, Train Acc: 0.8667, Test Acc: 0.8400\n",
      "Epoch: 058, Train Acc: 0.8647, Test Acc: 0.8380\n",
      "Epoch: 059, Train Acc: 0.8567, Test Acc: 0.8260\n",
      "Epoch: 060, Train Acc: 0.8593, Test Acc: 0.8340\n",
      "Epoch: 061, Train Acc: 0.8667, Test Acc: 0.8300\n",
      "Epoch: 062, Train Acc: 0.8653, Test Acc: 0.8380\n",
      "Epoch: 063, Train Acc: 0.8447, Test Acc: 0.8420\n",
      "Epoch: 064, Train Acc: 0.8787, Test Acc: 0.8500\n",
      "Epoch: 065, Train Acc: 0.8493, Test Acc: 0.8540\n",
      "Epoch: 066, Train Acc: 0.8713, Test Acc: 0.8540\n",
      "Epoch: 067, Train Acc: 0.8520, Test Acc: 0.8340\n",
      "Epoch: 068, Train Acc: 0.8600, Test Acc: 0.8360\n",
      "Epoch: 069, Train Acc: 0.8780, Test Acc: 0.8480\n",
      "Epoch: 070, Train Acc: 0.8713, Test Acc: 0.8440\n",
      "Epoch: 071, Train Acc: 0.8620, Test Acc: 0.8360\n",
      "Epoch: 072, Train Acc: 0.8753, Test Acc: 0.8680\n",
      "Epoch: 073, Train Acc: 0.8767, Test Acc: 0.8520\n",
      "Epoch: 074, Train Acc: 0.8760, Test Acc: 0.8440\n",
      "Epoch: 075, Train Acc: 0.8660, Test Acc: 0.8220\n",
      "Epoch: 076, Train Acc: 0.8653, Test Acc: 0.8600\n",
      "Epoch: 077, Train Acc: 0.8727, Test Acc: 0.8580\n",
      "Epoch: 078, Train Acc: 0.8700, Test Acc: 0.8600\n",
      "Epoch: 079, Train Acc: 0.8693, Test Acc: 0.8460\n",
      "Epoch: 080, Train Acc: 0.8760, Test Acc: 0.8380\n",
      "Epoch: 081, Train Acc: 0.8533, Test Acc: 0.8120\n",
      "Epoch: 082, Train Acc: 0.8640, Test Acc: 0.8620\n",
      "Epoch: 083, Train Acc: 0.8733, Test Acc: 0.8320\n",
      "Epoch: 084, Train Acc: 0.8707, Test Acc: 0.8600\n",
      "Epoch: 085, Train Acc: 0.8560, Test Acc: 0.8440\n",
      "Epoch: 086, Train Acc: 0.8827, Test Acc: 0.8600\n",
      "Epoch: 087, Train Acc: 0.8687, Test Acc: 0.8180\n",
      "Epoch: 088, Train Acc: 0.8820, Test Acc: 0.8540\n",
      "Epoch: 089, Train Acc: 0.8767, Test Acc: 0.8540\n",
      "Epoch: 090, Train Acc: 0.8747, Test Acc: 0.8420\n",
      "Epoch: 091, Train Acc: 0.8813, Test Acc: 0.8520\n",
      "Epoch: 092, Train Acc: 0.8593, Test Acc: 0.8280\n",
      "Epoch: 093, Train Acc: 0.8847, Test Acc: 0.8660\n",
      "Epoch: 094, Train Acc: 0.8940, Test Acc: 0.8560\n",
      "Epoch: 095, Train Acc: 0.8807, Test Acc: 0.8380\n",
      "Epoch: 096, Train Acc: 0.8947, Test Acc: 0.8620\n",
      "Epoch: 097, Train Acc: 0.8920, Test Acc: 0.8660\n",
      "Epoch: 098, Train Acc: 0.8933, Test Acc: 0.8500\n",
      "Epoch: 099, Train Acc: 0.8647, Test Acc: 0.8600\n",
      "Epoch: 100, Train Acc: 0.8860, Test Acc: 0.8540\n",
      "Epoch: 101, Train Acc: 0.8727, Test Acc: 0.8560\n",
      "Epoch: 102, Train Acc: 0.8693, Test Acc: 0.8540\n",
      "Epoch: 103, Train Acc: 0.8553, Test Acc: 0.8080\n",
      "Epoch: 104, Train Acc: 0.8933, Test Acc: 0.8720\n",
      "Epoch: 105, Train Acc: 0.8920, Test Acc: 0.8480\n",
      "Epoch: 106, Train Acc: 0.8933, Test Acc: 0.8620\n",
      "Epoch: 107, Train Acc: 0.8740, Test Acc: 0.8480\n",
      "Epoch: 108, Train Acc: 0.8933, Test Acc: 0.8720\n",
      "Epoch: 109, Train Acc: 0.8720, Test Acc: 0.8600\n",
      "Epoch: 110, Train Acc: 0.9007, Test Acc: 0.8720\n",
      "Epoch: 111, Train Acc: 0.8927, Test Acc: 0.8620\n",
      "Epoch: 112, Train Acc: 0.8940, Test Acc: 0.8480\n",
      "Epoch: 113, Train Acc: 0.8973, Test Acc: 0.8640\n",
      "Epoch: 114, Train Acc: 0.8820, Test Acc: 0.8780\n",
      "Epoch: 115, Train Acc: 0.8773, Test Acc: 0.8320\n",
      "Epoch: 116, Train Acc: 0.8553, Test Acc: 0.8520\n",
      "Epoch: 117, Train Acc: 0.8827, Test Acc: 0.8640\n",
      "Epoch: 118, Train Acc: 0.8953, Test Acc: 0.8640\n",
      "Epoch: 119, Train Acc: 0.8940, Test Acc: 0.8640\n",
      "Epoch: 120, Train Acc: 0.8973, Test Acc: 0.8580\n",
      "Epoch: 121, Train Acc: 0.8767, Test Acc: 0.8460\n",
      "Epoch: 122, Train Acc: 0.9013, Test Acc: 0.8780\n",
      "Epoch: 123, Train Acc: 0.8880, Test Acc: 0.8400\n",
      "Epoch: 124, Train Acc: 0.8993, Test Acc: 0.8760\n",
      "Epoch: 125, Train Acc: 0.8927, Test Acc: 0.8560\n",
      "Epoch: 126, Train Acc: 0.9007, Test Acc: 0.8480\n",
      "Epoch: 127, Train Acc: 0.8847, Test Acc: 0.8680\n",
      "Epoch: 128, Train Acc: 0.8907, Test Acc: 0.8660\n",
      "Epoch: 129, Train Acc: 0.9033, Test Acc: 0.8640\n",
      "Epoch: 130, Train Acc: 0.9080, Test Acc: 0.8560\n",
      "Epoch: 131, Train Acc: 0.9060, Test Acc: 0.8700\n",
      "Epoch: 132, Train Acc: 0.9027, Test Acc: 0.8540\n",
      "Epoch: 133, Train Acc: 0.8840, Test Acc: 0.8620\n",
      "Epoch: 134, Train Acc: 0.8973, Test Acc: 0.8760\n",
      "Epoch: 135, Train Acc: 0.8947, Test Acc: 0.8520\n",
      "Epoch: 136, Train Acc: 0.8947, Test Acc: 0.8340\n",
      "Epoch: 137, Train Acc: 0.9133, Test Acc: 0.8520\n",
      "Epoch: 138, Train Acc: 0.9093, Test Acc: 0.8640\n",
      "Epoch: 139, Train Acc: 0.8980, Test Acc: 0.8480\n",
      "Epoch: 140, Train Acc: 0.8993, Test Acc: 0.8440\n",
      "Epoch: 141, Train Acc: 0.8947, Test Acc: 0.8700\n",
      "Epoch: 142, Train Acc: 0.8940, Test Acc: 0.8700\n",
      "Epoch: 143, Train Acc: 0.8853, Test Acc: 0.8740\n",
      "Epoch: 144, Train Acc: 0.8920, Test Acc: 0.8560\n",
      "Epoch: 145, Train Acc: 0.9000, Test Acc: 0.8520\n",
      "Epoch: 146, Train Acc: 0.9120, Test Acc: 0.8660\n",
      "Epoch: 147, Train Acc: 0.8993, Test Acc: 0.8600\n",
      "Epoch: 148, Train Acc: 0.9067, Test Acc: 0.8420\n",
      "Epoch: 149, Train Acc: 0.9087, Test Acc: 0.8720\n",
      "Epoch: 150, Train Acc: 0.9000, Test Acc: 0.8740\n",
      "Epoch: 151, Train Acc: 0.9007, Test Acc: 0.8680\n",
      "Epoch: 152, Train Acc: 0.9040, Test Acc: 0.8640\n",
      "Epoch: 153, Train Acc: 0.9087, Test Acc: 0.8720\n",
      "Epoch: 154, Train Acc: 0.9120, Test Acc: 0.8800\n",
      "Epoch: 155, Train Acc: 0.9047, Test Acc: 0.8640\n",
      "Epoch: 156, Train Acc: 0.9020, Test Acc: 0.8720\n",
      "Epoch: 157, Train Acc: 0.8927, Test Acc: 0.8720\n",
      "Epoch: 158, Train Acc: 0.8773, Test Acc: 0.8580\n",
      "Epoch: 159, Train Acc: 0.9100, Test Acc: 0.8720\n",
      "Epoch: 160, Train Acc: 0.9160, Test Acc: 0.8780\n",
      "Epoch: 161, Train Acc: 0.9153, Test Acc: 0.8740\n",
      "Epoch: 162, Train Acc: 0.9033, Test Acc: 0.8720\n",
      "Epoch: 163, Train Acc: 0.9040, Test Acc: 0.8660\n",
      "Epoch: 164, Train Acc: 0.9200, Test Acc: 0.8720\n",
      "Epoch: 165, Train Acc: 0.8980, Test Acc: 0.8800\n",
      "Epoch: 166, Train Acc: 0.9187, Test Acc: 0.8680\n",
      "Epoch: 167, Train Acc: 0.9000, Test Acc: 0.8800\n",
      "Epoch: 168, Train Acc: 0.9147, Test Acc: 0.8680\n",
      "Epoch: 169, Train Acc: 0.9127, Test Acc: 0.8740\n",
      "Epoch: 170, Train Acc: 0.9093, Test Acc: 0.8520\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Javascript\n",
    "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
    "\n",
    "model = GCN(hidden_channels=64)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "         out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "         loss = criterion(out, data.y)  # Compute the loss.\n",
    "         loss.backward()  # Derive gradients.\n",
    "         optimizer.step()  # Update parameters based on gradients.\n",
    "         optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "def test(loader):\n",
    "     model.eval()\n",
    "\n",
    "     correct = 0\n",
    "     for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "         out = model(data.x, data.edge_index, data.batch)  \n",
    "         pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "         correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "     return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "\n",
    "\n",
    "for epoch in range(1, 171):\n",
    "    train()\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a42631",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329eacd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2fa12d5a",
   "metadata": {},
   "source": [
    "As mentioned above, in **GraphConv** we do the message passing a little different than the **GCNConv** layer:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_v^{(\\ell+1)} = \\mathbf{W}^{(\\ell + 1)}_1 \\mathbf{x}_v^{(\\ell)} + \\mathbf{W}^{(\\ell + 1)}_2 \\sum_{w \\in \\mathcal{N}(v)} \\mathbf{x}_w^{(\\ell)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "381e02df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN_graph_conv(\n",
      "  (conv1): GraphConv(38, 64)\n",
      "  (conv2): GraphConv(64, 64)\n",
      "  (conv3): GraphConv(64, 64)\n",
      "  (lin): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GraphConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "feature_size = dataset.num_node_features # here in this exampe x_feature is size of 38\n",
    "num_classes = dataset.num_classes\n",
    "\n",
    "\n",
    "class GCN_graph_conv(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN_graph_conv, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GraphConv(feature_size, hidden_channels)\n",
    "        self.conv2 = GraphConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GraphConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = GCN_graph_conv(hidden_channels=64)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3486c76e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Acc: 0.7773, Test Acc: 0.7740\n",
      "Epoch: 002, Train Acc: 0.7987, Test Acc: 0.8140\n",
      "Epoch: 003, Train Acc: 0.7980, Test Acc: 0.7920\n",
      "Epoch: 004, Train Acc: 0.8120, Test Acc: 0.8180\n",
      "Epoch: 005, Train Acc: 0.8060, Test Acc: 0.8080\n",
      "Epoch: 006, Train Acc: 0.8413, Test Acc: 0.8520\n",
      "Epoch: 007, Train Acc: 0.8747, Test Acc: 0.8240\n",
      "Epoch: 008, Train Acc: 0.8573, Test Acc: 0.8560\n",
      "Epoch: 009, Train Acc: 0.8807, Test Acc: 0.8560\n",
      "Epoch: 010, Train Acc: 0.9007, Test Acc: 0.8760\n",
      "Epoch: 011, Train Acc: 0.9020, Test Acc: 0.8640\n",
      "Epoch: 012, Train Acc: 0.8973, Test Acc: 0.8820\n",
      "Epoch: 013, Train Acc: 0.8940, Test Acc: 0.8740\n",
      "Epoch: 014, Train Acc: 0.9140, Test Acc: 0.9040\n",
      "Epoch: 015, Train Acc: 0.9207, Test Acc: 0.9000\n",
      "Epoch: 016, Train Acc: 0.9253, Test Acc: 0.8960\n",
      "Epoch: 017, Train Acc: 0.9253, Test Acc: 0.8820\n",
      "Epoch: 018, Train Acc: 0.9213, Test Acc: 0.8920\n",
      "Epoch: 019, Train Acc: 0.9153, Test Acc: 0.8900\n",
      "Epoch: 020, Train Acc: 0.9300, Test Acc: 0.8920\n",
      "Epoch: 021, Train Acc: 0.8960, Test Acc: 0.8500\n",
      "Epoch: 022, Train Acc: 0.9333, Test Acc: 0.9140\n",
      "Epoch: 023, Train Acc: 0.9407, Test Acc: 0.9160\n",
      "Epoch: 024, Train Acc: 0.9427, Test Acc: 0.9060\n",
      "Epoch: 025, Train Acc: 0.9547, Test Acc: 0.9120\n",
      "Epoch: 026, Train Acc: 0.9440, Test Acc: 0.9040\n",
      "Epoch: 027, Train Acc: 0.9447, Test Acc: 0.9060\n",
      "Epoch: 028, Train Acc: 0.9427, Test Acc: 0.9080\n",
      "Epoch: 029, Train Acc: 0.9267, Test Acc: 0.9020\n",
      "Epoch: 030, Train Acc: 0.9300, Test Acc: 0.9020\n",
      "Epoch: 031, Train Acc: 0.9320, Test Acc: 0.9100\n",
      "Epoch: 032, Train Acc: 0.9560, Test Acc: 0.9120\n",
      "Epoch: 033, Train Acc: 0.9520, Test Acc: 0.9120\n",
      "Epoch: 034, Train Acc: 0.9420, Test Acc: 0.9140\n",
      "Epoch: 035, Train Acc: 0.9540, Test Acc: 0.9140\n",
      "Epoch: 036, Train Acc: 0.9540, Test Acc: 0.9240\n",
      "Epoch: 037, Train Acc: 0.9580, Test Acc: 0.9020\n",
      "Epoch: 038, Train Acc: 0.9447, Test Acc: 0.9200\n",
      "Epoch: 039, Train Acc: 0.9573, Test Acc: 0.9100\n",
      "Epoch: 040, Train Acc: 0.9180, Test Acc: 0.9000\n",
      "Epoch: 041, Train Acc: 0.9280, Test Acc: 0.9140\n",
      "Epoch: 042, Train Acc: 0.9593, Test Acc: 0.9060\n",
      "Epoch: 043, Train Acc: 0.9627, Test Acc: 0.9140\n",
      "Epoch: 044, Train Acc: 0.9567, Test Acc: 0.9140\n",
      "Epoch: 045, Train Acc: 0.9600, Test Acc: 0.9140\n",
      "Epoch: 046, Train Acc: 0.9400, Test Acc: 0.8760\n",
      "Epoch: 047, Train Acc: 0.9660, Test Acc: 0.9220\n",
      "Epoch: 048, Train Acc: 0.9520, Test Acc: 0.9060\n",
      "Epoch: 049, Train Acc: 0.9633, Test Acc: 0.9020\n",
      "Epoch: 050, Train Acc: 0.9507, Test Acc: 0.9000\n",
      "Epoch: 051, Train Acc: 0.9607, Test Acc: 0.9140\n",
      "Epoch: 052, Train Acc: 0.9360, Test Acc: 0.8960\n",
      "Epoch: 053, Train Acc: 0.9467, Test Acc: 0.9080\n",
      "Epoch: 054, Train Acc: 0.9640, Test Acc: 0.9120\n",
      "Epoch: 055, Train Acc: 0.9440, Test Acc: 0.9060\n",
      "Epoch: 056, Train Acc: 0.9627, Test Acc: 0.9060\n",
      "Epoch: 057, Train Acc: 0.9713, Test Acc: 0.9200\n",
      "Epoch: 058, Train Acc: 0.9587, Test Acc: 0.9220\n",
      "Epoch: 059, Train Acc: 0.9653, Test Acc: 0.9160\n",
      "Epoch: 060, Train Acc: 0.9720, Test Acc: 0.9140\n",
      "Epoch: 061, Train Acc: 0.9640, Test Acc: 0.9140\n",
      "Epoch: 062, Train Acc: 0.9713, Test Acc: 0.9140\n",
      "Epoch: 063, Train Acc: 0.9467, Test Acc: 0.9180\n",
      "Epoch: 064, Train Acc: 0.9613, Test Acc: 0.9160\n",
      "Epoch: 065, Train Acc: 0.9707, Test Acc: 0.9200\n",
      "Epoch: 066, Train Acc: 0.9733, Test Acc: 0.9100\n",
      "Epoch: 067, Train Acc: 0.9707, Test Acc: 0.9120\n",
      "Epoch: 068, Train Acc: 0.9647, Test Acc: 0.9280\n",
      "Epoch: 069, Train Acc: 0.9713, Test Acc: 0.9060\n",
      "Epoch: 070, Train Acc: 0.9633, Test Acc: 0.9120\n",
      "Epoch: 071, Train Acc: 0.9693, Test Acc: 0.9100\n",
      "Epoch: 072, Train Acc: 0.9640, Test Acc: 0.9240\n",
      "Epoch: 073, Train Acc: 0.9620, Test Acc: 0.9160\n",
      "Epoch: 074, Train Acc: 0.9640, Test Acc: 0.9220\n",
      "Epoch: 075, Train Acc: 0.9533, Test Acc: 0.8940\n",
      "Epoch: 076, Train Acc: 0.9707, Test Acc: 0.8960\n",
      "Epoch: 077, Train Acc: 0.9760, Test Acc: 0.9180\n",
      "Epoch: 078, Train Acc: 0.9607, Test Acc: 0.9160\n",
      "Epoch: 079, Train Acc: 0.9553, Test Acc: 0.8960\n",
      "Epoch: 080, Train Acc: 0.9680, Test Acc: 0.9040\n",
      "Epoch: 081, Train Acc: 0.9713, Test Acc: 0.9160\n",
      "Epoch: 082, Train Acc: 0.9727, Test Acc: 0.9060\n",
      "Epoch: 083, Train Acc: 0.9780, Test Acc: 0.9180\n",
      "Epoch: 084, Train Acc: 0.9620, Test Acc: 0.9140\n",
      "Epoch: 085, Train Acc: 0.9720, Test Acc: 0.9080\n",
      "Epoch: 086, Train Acc: 0.9807, Test Acc: 0.9240\n",
      "Epoch: 087, Train Acc: 0.9740, Test Acc: 0.9160\n",
      "Epoch: 088, Train Acc: 0.9793, Test Acc: 0.9040\n",
      "Epoch: 089, Train Acc: 0.9767, Test Acc: 0.9140\n",
      "Epoch: 090, Train Acc: 0.9773, Test Acc: 0.9180\n",
      "Epoch: 091, Train Acc: 0.9793, Test Acc: 0.9120\n",
      "Epoch: 092, Train Acc: 0.9787, Test Acc: 0.9120\n",
      "Epoch: 093, Train Acc: 0.9833, Test Acc: 0.9180\n",
      "Epoch: 094, Train Acc: 0.9733, Test Acc: 0.9200\n",
      "Epoch: 095, Train Acc: 0.9633, Test Acc: 0.9020\n",
      "Epoch: 096, Train Acc: 0.9780, Test Acc: 0.9240\n",
      "Epoch: 097, Train Acc: 0.9687, Test Acc: 0.9180\n",
      "Epoch: 098, Train Acc: 0.9760, Test Acc: 0.9240\n",
      "Epoch: 099, Train Acc: 0.9767, Test Acc: 0.9180\n",
      "Epoch: 100, Train Acc: 0.9660, Test Acc: 0.9180\n",
      "Epoch: 101, Train Acc: 0.9507, Test Acc: 0.9160\n",
      "Epoch: 102, Train Acc: 0.9693, Test Acc: 0.9200\n",
      "Epoch: 103, Train Acc: 0.9833, Test Acc: 0.9160\n",
      "Epoch: 104, Train Acc: 0.9800, Test Acc: 0.9160\n",
      "Epoch: 105, Train Acc: 0.9773, Test Acc: 0.9180\n",
      "Epoch: 106, Train Acc: 0.9840, Test Acc: 0.9260\n",
      "Epoch: 107, Train Acc: 0.9793, Test Acc: 0.9240\n",
      "Epoch: 108, Train Acc: 0.9873, Test Acc: 0.9300\n",
      "Epoch: 109, Train Acc: 0.9600, Test Acc: 0.9200\n",
      "Epoch: 110, Train Acc: 0.9847, Test Acc: 0.9260\n",
      "Epoch: 111, Train Acc: 0.9860, Test Acc: 0.9240\n",
      "Epoch: 112, Train Acc: 0.9827, Test Acc: 0.9300\n",
      "Epoch: 113, Train Acc: 0.9847, Test Acc: 0.9220\n",
      "Epoch: 114, Train Acc: 0.9793, Test Acc: 0.9240\n",
      "Epoch: 115, Train Acc: 0.9840, Test Acc: 0.9180\n",
      "Epoch: 116, Train Acc: 0.9820, Test Acc: 0.9220\n",
      "Epoch: 117, Train Acc: 0.9820, Test Acc: 0.9140\n",
      "Epoch: 118, Train Acc: 0.9813, Test Acc: 0.9220\n",
      "Epoch: 119, Train Acc: 0.9827, Test Acc: 0.9120\n",
      "Epoch: 120, Train Acc: 0.9680, Test Acc: 0.9140\n",
      "Epoch: 121, Train Acc: 0.9787, Test Acc: 0.9200\n",
      "Epoch: 122, Train Acc: 0.9713, Test Acc: 0.9140\n",
      "Epoch: 123, Train Acc: 0.9467, Test Acc: 0.8920\n",
      "Epoch: 124, Train Acc: 0.9767, Test Acc: 0.9240\n",
      "Epoch: 125, Train Acc: 0.9740, Test Acc: 0.9260\n",
      "Epoch: 126, Train Acc: 0.9520, Test Acc: 0.9060\n",
      "Epoch: 127, Train Acc: 0.9733, Test Acc: 0.9200\n",
      "Epoch: 128, Train Acc: 0.9807, Test Acc: 0.9140\n",
      "Epoch: 129, Train Acc: 0.9840, Test Acc: 0.9200\n",
      "Epoch: 130, Train Acc: 0.9793, Test Acc: 0.8960\n",
      "Epoch: 131, Train Acc: 0.9853, Test Acc: 0.9120\n",
      "Epoch: 132, Train Acc: 0.9887, Test Acc: 0.9180\n",
      "Epoch: 133, Train Acc: 0.9840, Test Acc: 0.9240\n",
      "Epoch: 134, Train Acc: 0.9867, Test Acc: 0.9200\n",
      "Epoch: 135, Train Acc: 0.9867, Test Acc: 0.9240\n",
      "Epoch: 136, Train Acc: 0.9880, Test Acc: 0.9220\n",
      "Epoch: 137, Train Acc: 0.9813, Test Acc: 0.9220\n",
      "Epoch: 138, Train Acc: 0.9860, Test Acc: 0.9220\n",
      "Epoch: 139, Train Acc: 0.9867, Test Acc: 0.9320\n",
      "Epoch: 140, Train Acc: 0.9780, Test Acc: 0.9200\n",
      "Epoch: 141, Train Acc: 0.9887, Test Acc: 0.9220\n",
      "Epoch: 142, Train Acc: 0.9873, Test Acc: 0.9180\n",
      "Epoch: 143, Train Acc: 0.9880, Test Acc: 0.9280\n",
      "Epoch: 144, Train Acc: 0.9827, Test Acc: 0.9220\n",
      "Epoch: 145, Train Acc: 0.9880, Test Acc: 0.9320\n",
      "Epoch: 146, Train Acc: 0.9887, Test Acc: 0.9300\n",
      "Epoch: 147, Train Acc: 0.9873, Test Acc: 0.9220\n",
      "Epoch: 148, Train Acc: 0.9787, Test Acc: 0.9240\n",
      "Epoch: 149, Train Acc: 0.9807, Test Acc: 0.9120\n",
      "Epoch: 150, Train Acc: 0.9873, Test Acc: 0.9200\n",
      "Epoch: 151, Train Acc: 0.9827, Test Acc: 0.9280\n",
      "Epoch: 152, Train Acc: 0.9853, Test Acc: 0.9220\n",
      "Epoch: 153, Train Acc: 0.9860, Test Acc: 0.9240\n",
      "Epoch: 154, Train Acc: 0.9860, Test Acc: 0.9240\n",
      "Epoch: 155, Train Acc: 0.9927, Test Acc: 0.9280\n",
      "Epoch: 156, Train Acc: 0.9827, Test Acc: 0.9160\n",
      "Epoch: 157, Train Acc: 0.9833, Test Acc: 0.9320\n",
      "Epoch: 158, Train Acc: 0.9860, Test Acc: 0.9240\n",
      "Epoch: 159, Train Acc: 0.9880, Test Acc: 0.9260\n",
      "Epoch: 160, Train Acc: 0.9847, Test Acc: 0.9220\n",
      "Epoch: 161, Train Acc: 0.9860, Test Acc: 0.9140\n",
      "Epoch: 162, Train Acc: 0.9853, Test Acc: 0.9240\n",
      "Epoch: 163, Train Acc: 0.9800, Test Acc: 0.9260\n",
      "Epoch: 164, Train Acc: 0.9893, Test Acc: 0.9260\n",
      "Epoch: 165, Train Acc: 0.9693, Test Acc: 0.9340\n",
      "Epoch: 166, Train Acc: 0.9853, Test Acc: 0.9220\n",
      "Epoch: 167, Train Acc: 0.9820, Test Acc: 0.9260\n",
      "Epoch: 168, Train Acc: 0.9880, Test Acc: 0.9180\n",
      "Epoch: 169, Train Acc: 0.9900, Test Acc: 0.9220\n",
      "Epoch: 170, Train Acc: 0.9807, Test Acc: 0.9200\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Javascript\n",
    "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
    "\n",
    "model = GCN_graph_conv(hidden_channels=64)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "         out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "         loss = criterion(out, data.y)  # Compute the loss.\n",
    "         loss.backward()  # Derive gradients.\n",
    "         optimizer.step()  # Update parameters based on gradients.\n",
    "         optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "def test(loader):\n",
    "     model.eval()\n",
    "\n",
    "     correct = 0\n",
    "     for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "         out = model(data.x, data.edge_index, data.batch)  \n",
    "         pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "         correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "     return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "\n",
    "\n",
    "for epoch in range(1, 171):\n",
    "    train()\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ea7dc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e8fa95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c955d1a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53c62cf9",
   "metadata": {},
   "source": [
    "# Reference:\n",
    "https://colab.research.google.com/drive/1I8a0DfQ3fI7Njc62__mVXUlcAleUclnb?usp=sharing#scrollTo=HvhgQoO8Svw4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4825e309",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
